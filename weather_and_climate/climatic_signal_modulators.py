#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#----------------#
# Import modules #
#----------------#

import numpy as np
import scipy.signal as ssig

#------------------#
# Define functions #
#------------------#

def autocorrelate(x, twosided=False):
    
    """
    Function that calculates the autocorrelation of a time series.
    
    Adapted from https://www.investopedia.com/terms/a/autocorrelation.asp
    ---------------------------------------------------------------------
    Autocorrelation is a mathematical representation
    of the degree of similarity between a given time series
    and a lagged version of itself over successive time intervals.
    
    It's conceptually similar to the correlation between
    two different time series,
    but autocorrelation uses the same time series twice:
    Once in its original form and once lagged one or more time periods. 
    
    For example, if it's rainy today, the data suggests that it's more likely
    to rain tomorrow than if it's clear today.
    When it comes to investing, a stock might have a
    strong positive autocorrelation of returns,
    suggesting that if it's "up" today,
    it's more likely to be up tomorrow, too.
    
    Of course, autocorrelation can be a useful tool for traders to utilize;
    particularly for technical analysts.
      
    Adapted from https://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation
    -----------------------------------------------------------------------------------------------------------
    
    At a time difference of 0,
    the auto-correlation should be the highest
    because the signal is identical to itself,
    so it its expected that the first element in the autocorrelation
    result array would be the greatest.
    
    The definition implies sums for all lags (not strictly times),
    so it is performed from tau in [-inf, inf] (two sided).
    Usually one is only interested on the series from lags
    tau in [0, inf].
    
    Apart from these explanations, it is important to notice that,
    the definition involves the calculation of the expected value
    of the simple deviation, i.e. value at lag tau minus the series mean.
    
    The function uses numpy.correlate module by default.
    
    Parameters
    ----------
    x : list or numpy.array
          Object containing certain variable's data.
    twosided : bool
          If it is False, then only autocorrelation ranging from 
          lags [0, infinity] is calculated,
          otherwise from lags [-infinity, infinity].
    
    Returns
    -------
    x_autocorr : numpy.ndarray
          Array containing autocorrelation values.
    
    Note
    ----
    Checked experimentally. For large arrays (>~75000) according to
    numpy documentation, it is preferable to use scipy.signal.correlate
    since it uses Fourier methods, which makes the computation way faster.
    NEITHER numpy.correlate nor scipy.signal.correlate ignore NaN values,
    so they have to be removed previously.
    """
    
    x_nonan = x[~np.isnan(x)]
    x_demean = x_nonan - np.mean(x_nonan)
    lx = len(x_demean)
    
    if lx <= int(5e4):
        x_autocorr = np.correlate(x_demean, x_demean, mode="full")
    else:
        x_autocorr = ssig.correlate(x_demean, x_demean)
    
    x_autocorr_norm = x_autocorr / np.max(x_autocorr)
    
    if twosided:
        return x_autocorr_norm
    else:
        x_autocorr_onesided = x_autocorr_norm[x_autocorr_norm.size//2:] 
        return x_autocorr_onesided
    
    
def polynomial_fitting(y, poly_ord, fix_edges=False, 
                       poly_func=None, poly_params=None):
    
    """
    Performs the polynomial fitting over a 1D array,
    using the least squares fit method.
    
    This fitting method tries to fit a curve with a polynomial
    p(x) = p[0] * x**deg + ... + p[deg] of degree 'deg' to points (x, y).
    
    For that, returns a vector of coefficients p that minimises
    the squared error in the order deg, deg-1, ... 0,
    
    A lot of examples include pre-selected x-points that are passed into
    function numpy.polyfit, but as far as I am concerned,
    it is more practical to simply construct a linear space,
    between the edges of the array to be fitted.
    
    Having this vector of coefficients, it then emulates the
    mentioned polynomial to encapsulate "natural" operations on polynomials.
    In simpler words, it constructs the polynomial as in written form, 
    that is, including the unknown variable.
    In this way, the array is fitted by evaluating it over the
    best fitting polynomial.
    
    Parameters
    ----------
    y : list or np.ndarray
          y-coordinates of the sample points. It must be 1D, otherwise
          the function flattens the object to 1D.
    poly_ord : int 
          The order of the array-fitting polynomial
    fix_edges : bool
          If True, after performing the interpolation, the original
          edges of array 'y' will be maintained,
          for that making the following substitution:
              路new_y[0] = y[0]      
              路new_y[-1] = y[-1] 
    
    poly_func : function
          An alternative that consists of a function
          with a predefined polynomial.
          If is not None, then  the'scipy.optimize' module's 'curve_fit' 
          function is used.
          An example of this function would be, 
          for the case of a 4th order polynomial:
    
              路def func(x, a, b, c, d, e):
                    return a*x**3 + b*x**2 + c*x + d,
          
          where 'x' is a vector.
    
    poly_params : list of integers or dictionary of integers
          Used to pass the parameters into the latter.
          An example linked with the mentioned 'func' function would be:
    
          路params = dict(a=0.6, b=0.15, c=0.2, d=0.7)
    
    Returns
    -------
    new_y : np.ndarray
            Polynomial fitted value containing array.
    
    Note
    ----
    The inconvenience of choosing the 'curve_fit' method it that the user
    has to define the function manually and adjust the parameters
    depending on the quality of the curve fitting.
    """
    
    y_shape = y.shape
    if len(y_shape) > 1:
        y = y.flatten()
    
    ly = len(y)
    x = np.arange(ly)
    
    if poly_func is None:
        x_linspace = np.linspace(x[0], x[-1])

        coefficients = np.polyfit(x, y, poly_ord)
        polynomial = np.poly1d(coefficients)
        new_y = polynomial(x_linspace)
        
    else:
        from scipy.optimize import curve_fit
        
        y_polynomial = poly_func(y, **poly_params)
        popt, pcov = curve_fit(poly_func, x, y_polynomial)
        new_y = poly_func(x,*popt)
        
    if fix_edges:
        new_y_fixed_edges = new_y.copy()
        new_y_fixed_edges[0] = y[0]
        new_y_fixed_edges[-1] = y[-1]
        
        return new_y_fixed_edges
    
    else:
        return new_y
    
# TODO: definitu seinalea zuritzeko funtzioak
# def signal_whitening(data, method="classic"):
    
#     if method=="classic":
        
#     elif method=="sklearn":
        
#     elif method=="zca":
